import streamlit as st
import pandas as pd
import os
import re
import difflib
import io

# ================= ç½‘é¡µé…ç½® =================
st.set_page_config(page_title="LCA æ™ºèƒ½åŒ¹é…ç³»ç»Ÿ", page_icon="ğŸŒ±", layout="wide")

st.title("ğŸŒ± LCA æ™ºèƒ½åŒ¹é…ç³»ç»Ÿ (Webç‰ˆ)")
st.markdown("""
### ä½¿ç”¨è¯´æ˜
1. è¯·ç¡®ä¿åå°å·²åŠ è½½æ‰€æœ‰åŸºç¡€æ•°æ®åº“ï¼ˆä¸Šæ¸¸ã€åŸºæœ¬æµã€åºŸå¼ƒç‰©ç­‰ï¼‰ã€‚
2. ä¸Šä¼ ä½ çš„ **äº§å“æ¨¡å‹ç‰©æ–™è¡¨**ã€‚
3. ç³»ç»Ÿå°†è‡ªåŠ¨åŒ¹é…å¹¶ç”Ÿæˆæ ‡å‡†æ ¼å¼çš„ CSV ä¾›ä¸‹è½½ã€‚
""")

# ================= 0. åå°æ–‡ä»¶åŠ è½½å™¨ =================
@st.cache_data # ç¼“å­˜æœºåˆ¶ï¼Œè®©å‚è€ƒè¡¨åªåŠ è½½ä¸€æ¬¡ï¼Œä¸ç”¨æ¯æ¬¡åˆ·æ–°éƒ½è¯»
def load_reference_data():
    # è¿™é‡Œå¡«å†™ä½ æ”¾åœ¨æ–‡ä»¶å¤¹é‡Œçš„çœŸå®æ–‡ä»¶å
    files = {
        "å¤´è¡¨": "åŒ¹é…å…³ç³»å¤´è¡¨.CSV", 
        "ä¸Šæ¸¸è¡¨": "åŒ¹é…å…³ç³»ä¸Šæ¸¸èƒŒæ™¯æ•°æ®è¡Œè¡¨.CSV",
        "åŸºæœ¬æµè¡¨": "åŒ¹é…å…³ç³»åŸºæœ¬æµè¡¨.CSV",
        "åºŸå¼ƒç‰©è¡¨": "åŒ¹é…å…³ç³»åºŸå¼ƒç‰©å¤„ç½®èƒŒæ™¯æ•°æ®è¡Œè¡¨.CSV",
        "å‰¯äº§å“è¡¨": "åŒ¹é…å…³ç³»å‰¯äº§å“èƒŒæ™¯æ•°æ®è¡Œè¡¨.CSV",
        "å›æ”¶åˆ©ç”¨è¡¨": "åŒ¹é…å…³ç³»å›æ”¶åˆ©ç”¨èƒŒæ™¯æ•°æ®è¡Œè¡¨.CSV"
    }
    
    loaded_dfs = {}
    missing_files = []

    for key, filename in files.items():
        if os.path.exists(filename):
            try:
                loaded_dfs[key] = pd.read_csv(filename, dtype=str)
            except:
                try:
                    loaded_dfs[key] = pd.read_csv(filename, encoding='gbk', dtype=str)
                except:
                    loaded_dfs[key] = pd.read_excel(filename, dtype=str)
        else:
            missing_files.append(filename)
    
    return loaded_dfs, missing_files

# åŠ è½½å‚è€ƒæ•°æ®
ref_dfs, missing = load_reference_data()

# ä¾§è¾¹æ æ˜¾ç¤ºçŠ¶æ€
st.sidebar.header("ğŸ“¦ æ•°æ®åº“çŠ¶æ€")
if missing:
    st.sidebar.error(f"âŒ ç¼ºå¤±æ–‡ä»¶: {missing}")
    st.error("åå°å‚è€ƒæ–‡ä»¶ç¼ºå¤±ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ä¸Šä¼ ï¼")
    st.stop()
else:
    st.sidebar.success(f"âœ… å·²åŠ è½½ {len(ref_dfs)} ä¸ªå‚è€ƒæ•°æ®åº“")

# ================= 1. æ ¸å¿ƒé€»è¾‘ (V38å°è£…) =================

def process_matching(df_model, ref_dfs):
    # è§£åŒ…æ•°æ®
    df_header = ref_dfs['å¤´è¡¨']
    bg_dfs = {
        'Upstream': ref_dfs['ä¸Šæ¸¸è¡¨'], 'Waste': ref_dfs['åºŸå¼ƒç‰©è¡¨'],
        'Byprod': ref_dfs['å‰¯äº§å“è¡¨'], 'Recycle': ref_dfs['å›æ”¶åˆ©ç”¨è¡¨'], 'Elementary': ref_dfs['åŸºæœ¬æµè¡¨']
    }

    # --- æ„å»ºç´¢å¼• ---
    h_name_col = next((c for c in df_header.columns if 'åç§°' in c and 'ä¸­æ–‡' in c), 'ç‰©æ–™é¡¹åç§°ï¼ˆä¸­æ–‡ï¼‰')
    h_id_col = next((c for c in df_header.columns if 'åŒ¹é…å…³ç³»ID' in c), 'åŒ¹é…å…³ç³»ID')
    df_header['clean'] = df_header[h_name_col].astype(str).str.strip()
    header_map = df_header.set_index('clean')[h_id_col].astype(str).str.strip().to_dict()
    
    bg_id_map = {}
    bg_name_list = {'Upstream': [], 'Waste': [], 'Byprod': [], 'Recycle': [], 'Elementary': []}
    
    # å»ºç«‹è¿›åº¦æ¡
    progress_text = "æ­£åœ¨ç´¢å¼•èƒŒæ™¯æ•°æ®åº“..."
    my_bar = st.progress(0, text=progress_text)
    
    total_cats = len(bg_dfs)
    current_cat_idx = 0

    for cat, df in bg_dfs.items():
        lid_col = next((c for c in df.columns if 'åŒ¹é…å…³ç³»ID' in c), None)
        if cat == 'Elementary':
            name_col = next((c for c in df.columns if 'åŸºæœ¬æµåç§°' in c and 'ä¸­æ–‡' in c), 'åŸºæœ¬æµåç§°ï¼ˆä¸­æ–‡ï¼‰')
            unit_col = next((c for c in df.columns if 'å•ä½' in c), 'å•ä½ï¼ˆè‹±æ–‡ï¼‰')
            loc_col = next((c for c in df.columns if 'åˆ†ç±»' in c), 'åŸºæœ¬æµåˆ†ç±»') 
            fact_col, ref_col = None, None
        else:
            name_col = next((c for c in df.columns if 'åç§°' in c and 'ä¸­æ–‡' in c), 'åç§°')
            unit_col = next((c for c in df.columns if 'å•ä½' in c), 'å•ä½')
            loc_col = next((c for c in df.columns if 'åœ°ç†ä½ç½®' in c), 'åœ°ç†ä½ç½®')
            fact_col = next((c for c in df.columns if 'ç¢³è¶³è¿¹' in c), 'ç¢³è¶³è¿¹')
            ref_col = next((c for c in df.columns if 'å‚è€ƒäº§å“' in c), None)
        
        id_col = 'ID'
        for _, row in df.iterrows():
            item = {
                'ID': str(row.get(id_col, '')).strip(),
                'ç¢³è¶³è¿¹': str(row.get(fact_col, '')) if fact_col else "N/A",
                'å•ä½': str(row.get(unit_col, '')).strip(),
                'åœ°ç†ä½ç½®': str(row.get(loc_col, '')).strip(),
                'èƒŒæ™¯åç§°': str(row.get(name_col, '')).strip(),
                'å‚è€ƒäº§å“': str(row.get(ref_col, '')).strip() if ref_col else "",
                'æ¥æº': cat
            }
            if lid_col:
                lid = str(row[lid_col]).strip()
                if lid not in bg_id_map: bg_id_map[lid] = []
                bg_id_map[lid].append(item)
            bg_name_list[cat].append(item)
        
        current_cat_idx += 1
        my_bar.progress(int(current_cat_idx / total_cats * 20), text="æ­£åœ¨ç´¢å¼•èƒŒæ™¯æ•°æ®åº“...")

    # --- å·¥å…·å‡½æ•° ---
    STRICT_LOCATIONS = {
        'ä¸­å›½', 'cn', 'china', 'å…¨çƒ', 'glo', 'global',
        'row', 'rest of world', 'ä¸–ç•Œå…¶ä»–åœ°åŒº', 'æœªæŒ‡å®š', 'unspecified'
    }
    SYNONYMS_MAP = {
        'æ²³æ°´': ['åœ°è¡¨æ°´', 'surface water', 'water, river', 'æ²³', 'river'],
        'æ¹–æ°´': ['åœ°è¡¨æ°´', 'surface water', 'water, lake', 'æ¹–', 'lake'],
        'é›¨æ°´': ['åœ°è¡¨æ°´', 'surface water', 'water, rain', 'é›¨'],
        'å†·å´æ°´': ['è‡ªæ¥æ°´', 'tap water'], 'å¾ªç¯æ°´': ['è‡ªæ¥æ°´', 'tap water']
    }
    SPECIAL_RULES = {'ä¸€èˆ¬å·¥ä¸šå›ºåºŸ': '43274789141377048'}

    def clean_name_str(s):
        return re.sub(r'\(.*?\)|ï¼ˆ.*?ï¼‰', '', s).strip()

    def string_similarity(s1, s2):
        return difflib.SequenceMatcher(None, s1.lower(), s2.lower()).ratio()

    def check_unit(m_unit, bg_unit):
        if not m_unit or not bg_unit: return ""
        return "ä¸€è‡´" if m_unit == bg_unit else "ä¸ä¸€è‡´"

    def get_score(item, m_name, m_cat):
        loc = item['åœ°ç†ä½ç½®']
        bg_name = item['èƒŒæ™¯åç§°'].lower()
        ref_prod = item['å‚è€ƒäº§å“'].lower()
        source = item['æ¥æº']
        m_name_clean = clean_name_str(m_name).lower()
        
        if 'å†·å´æ°´' in m_name or 'å¾ªç¯æ°´' in m_name:
            if ('è‡ªæ¥æ°´' in bg_name or 'tap water' in bg_name):
                return 999 if ('å¸‚åœº' in bg_name or 'market' in bg_name) else 500
        
        score = 10 
        if source == 'Elementary':
            score = 50
            if 'æœªæŒ‡å®š' in loc or 'unspecified' in loc.lower(): score += 30
            if 'æ°´' in m_name:
                if 'æœªæŒ‡å®šçš„å¤©ç„¶æ¥æº' in bg_name or 'unspecified natural origin' in bg_name: score += 20
                if 'åœ°è¡¨' in loc or 'surface' in loc.lower(): score += 15
                if 'æ²³' in m_name and 'river' in bg_name: score += 40 
                if 'æ¹–' in m_name and 'lake' in bg_name: score += 40 
            if m_cat == 'å¤§æ°”æ’æ”¾' and 'ç©ºæ°”' in loc: score += 10
            elif m_cat == 'æ°´ä½“æ’æ”¾' and 'æ°´' in loc: score += 10
            sim = string_similarity(m_name, item['èƒŒæ™¯åç§°'])
            score += sim * 5
            return score
        
        if 'hiq' in bg_name and loc=='ä¸­å›½': score = 100
        elif loc=='ä¸­å›½': score = 90
        elif 'ä¸–ç•Œå…¶ä»–åœ°åŒº' in loc or 'RoW' in loc: score = 80
        elif 'å…¨çƒ' in loc: score = 70
        
        if len(ref_prod) > 1 and (m_name_clean in ref_prod or ref_prod in m_name_clean): score += 20
        if any(k in bg_name for k in ['æœªæŒ‡å®š','unspecified','ä¸æŒ‡å®š','å¹³å‡','é€šç”¨','æ··åˆ']): score += 25
        if any(k in bg_name for k in ['ç”Ÿäº§','production','åˆ¶é€ ']): score += 10
        if m_cat in ['åºŸå¼ƒç‰©', 'å‰¯äº§å“']:
            whitelist = ['å¤„ç†','å¤„ç½®','ç„šçƒ§','å¡«åŸ‹','å›æ”¶','å†åˆ©ç”¨','treatment','disposal']
            if 'ç”Ÿäº§' in bg_name and not any(w in bg_name for w in whitelist): score -= 40
        
        sim = string_similarity(m_name_clean, clean_name_str(item['èƒŒæ™¯åç§°']).lower())
        score += sim * 10 
        return score

    # --- ä¸»å¾ªç¯ ---
    my_bar.progress(30, text="æ­£åœ¨æ‰§è¡Œæ™ºèƒ½åŒ¹é…...")
    result_data = []
    
    total_rows = len(df_model)
    
    for idx, row in df_model.iterrows():
        # æ›´æ–°è¿›åº¦æ¡
        if idx % 10 == 0:
            prog = 30 + int((idx / total_rows) * 60)
            my_bar.progress(prog, text=f"æ­£åœ¨å¤„ç†ç¬¬ {idx+1}/{total_rows} è¡Œ...")

        m_name = str(row.get('ç‰©æ–™é¡¹åç§°ï¼ˆä¸­æ–‡ï¼‰', '')).strip()
        m_cat = str(row.get('ç‰©æ–™é¡¹ç±»åˆ«', '')).strip()
        m_type = str(row.get('ç‰©æ–™é¡¹ç±»å‹', ''))
        m_attr = str(row.get('ç‰©æ–™é¡¹å±æ€§', ''))
        
        candidates = []
        
        if m_name in SPECIAL_RULES:
            cands = bg_id_map.get(SPECIAL_RULES[m_name])
            if cands: candidates.extend(cands)
        
        lid = header_map.get(m_name)
        if lid and lid in bg_id_map:
            cands = bg_id_map[lid]
            if m_cat in ['è‡ªç„¶èµ„æºè¾“å…¥', 'å¤§æ°”æ’æ”¾', 'æ°´ä½“æ’æ”¾']:
                candidates.extend([c for c in cands if c['æ¥æº'] == 'Elementary'])
            else:
                candidates.extend([c for c in cands if c['æ¥æº'] != 'Elementary'])
        
        search_terms = [m_name, clean_name_str(m_name)]
        if m_name in SYNONYMS_MAP: search_terms.extend(SYNONYMS_MAP[m_name])
        
        target_cats = []
        if m_cat in ['åŸè¾…æ–™', 'èƒ½æºåŠèƒ½æºä»‹è´¨']: target_cats = ['Upstream']
        elif m_cat == 'åºŸå¼ƒç‰©': target_cats = ['Waste']
        elif m_cat == 'å‰¯äº§å“': target_cats = ['Byprod']
        elif m_cat == 'å›æ”¶åˆ©ç”¨': target_cats = ['Recycle']
        elif m_cat in ['è‡ªç„¶èµ„æºè¾“å…¥', 'å¤§æ°”æ’æ”¾', 'æ°´ä½“æ’æ”¾']: target_cats = ['Elementary']
        
        is_natural = any(x in m_name for x in ['æ°´', 'æ²³', 'æ¹–', 'é›¨', 'äº•', 'æ°”', 'åœŸ', 'èµ„æº'])
        if is_natural or not candidates:
            if 'Elementary' not in target_cats: target_cats.append('Elementary')

        for cat in target_cats:
            for item in bg_name_list[cat]:
                bg_name = item['èƒŒæ™¯åç§°'].lower()
                for term in search_terms:
                    if term.lower() in bg_name:
                        candidates.append(item)
                        break

        if candidates:
            unique_candidates = {c['ID']: c for c in candidates}.values()
            filtered_candidates = [c for c in unique_candidates if str(c['åœ°ç†ä½ç½®']).strip().lower() in STRICT_LOCATIONS]
            candidates = filtered_candidates
            candidates.sort(key=lambda x: get_score(x, m_name, m_cat), reverse=True)
            
            for i, cand in enumerate(candidates):
                is_default = (i == 0)
                is_hiq = 'hiq' in cand['èƒŒæ™¯åç§°'].lower()
                row_data = [
                    m_type if is_default else "", m_attr if is_default else "", m_name if is_default else "",
                    cand['ID'],
                    "" if is_hiq else cand['èƒŒæ™¯åç§°'], "" if is_hiq else cand['å‚è€ƒäº§å“'],
                    "" if is_hiq else cand['åœ°ç†ä½ç½®'], "" if is_hiq else cand['å•ä½'],
                    cand['èƒŒæ™¯åç§°'] if is_hiq else "", cand['å‚è€ƒäº§å“'] if is_hiq else "",
                    cand['åœ°ç†ä½ç½®'] if is_hiq else "", cand['å•ä½'] if is_hiq else "",
                    "", ""
                ]
                result_data.append(row_data)
        else:
            row_data = [m_type, m_attr, m_name, "âŒ æ— åŒ¹é…", "", "", "", "", "", "", "", "", "", ""]
            result_data.append(row_data)

    my_bar.progress(100, text="å¤„ç†å®Œæˆï¼")
    
    # æ„å»º DataFrame
    FINAL_HEADERS = [
        'ç‰©æ–™é¡¹ç±»å‹', 'ç‰©æ–™é¡¹å±æ€§', '*ç‰©æ–™é¡¹åç§°ï¼ˆä¸­æ–‡ï¼‰', 'èƒŒæ™¯æ•°æ®/åŸºæœ¬æµID\nï¼ˆç‰¹æ®Šç‰©æ–™é¡¹éœ€åŒ¹é…ç‰¹æ®Šèµ„æºæŒ‡æ ‡ï¼‰',
        'é»˜è®¤èƒŒæ™¯æ•°æ®åç§°(ECO)\nï¼ˆæµåç§°è‹±æ–‡ï¼‰\nï¼ˆé¦–é€‰é‚£æ¡é«˜äº®æ˜¾ç¤ºï¼‰', 'å‚è€ƒäº§å“\nï¼ˆåŸºæœ¬æµåˆ†ç±»ï¼‰', 'åœ°åŒº', 'èƒŒæ™¯æ•°æ®å•ä½',
        'é»˜è®¤èƒŒæ™¯æ•°æ®åç§°(HIQ)\nï¼ˆé¦–é€‰é‚£æ¡é«˜äº®æ˜¾ç¤ºï¼‰', 'å‚è€ƒäº§å“', 'åœ°åŒº', 'èƒŒæ™¯æ•°æ®å•ä½',
        'è´Ÿè´£äºº', 'å®¡æ ¸æ„è§'
    ]
    df_out = pd.DataFrame(result_data)
    return df_out, FINAL_HEADERS

# ================= 2. ç”¨æˆ·äº¤äº’ç•Œé¢ =================

uploaded_file = st.file_uploader("ğŸ“‚ è¯·ä¸Šä¼  [æ¨¡å‹ç‰©æ–™é¡¹] è¡¨æ ¼ (CSVæˆ–Excel)", type=['csv', 'xlsx'])

if uploaded_file is not None:
    try:
        if uploaded_file.name.endswith('.csv'):
            df_input = pd.read_csv(uploaded_file, dtype=str)
        else:
            df_input = pd.read_excel(uploaded_file, dtype=str)
        
        st.info(f"æˆåŠŸåŠ è½½æ–‡ä»¶ï¼ŒåŒ…å« {len(df_input)} è¡Œæ•°æ®ã€‚ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¼€å§‹åŒ¹é…ã€‚")
        
        if st.button("ğŸš€ å¼€å§‹æ™ºèƒ½åŒ¹é…"):
            with st.spinner('AI æ­£åœ¨å…¨åŠ›è®¡ç®—ä¸­...'):
                df_result, headers = process_matching(df_input, ref_dfs)
            
            st.success("ğŸ‰ åŒ¹é…å®Œæˆï¼")
            
            # é¢„è§ˆç»“æœ
            st.markdown("### ğŸ“Š ç»“æœé¢„è§ˆ")
            # ä¸´æ—¶é‡å‘½ååˆ—ä»¥ä¾¿é¢„è§ˆ
            preview_df = df_result.copy()
            preview_df.columns = [h.replace('\n', ' ') for h in headers]
            st.dataframe(preview_df.head(50))
            
            # ä¸‹è½½æŒ‰é’®
            # ä¸ºäº†ä¿è¯CSVè¡¨å¤´æ­£ç¡®ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å¤„ç†
            csv_buffer = io.StringIO()
            # å†™å…¥è¡¨å¤´
            import csv
            writer = csv.writer(csv_buffer)
            writer.writerow(headers)
            # å†™å…¥æ•°æ®
            for _, row in df_result.iterrows():
                writer.writerow(row.tolist())
                
            st.download_button(
                label="ğŸ’¾ ä¸‹è½½æœ€ç»ˆç»“æœ CSV",
                data=csv_buffer.getvalue().encode('utf-8-sig'),
                file_name="LCA_åŒ¹é…ç»“æœ_V38.csv",
                mime="text/csv"
            )

    except Exception as e:
        st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")