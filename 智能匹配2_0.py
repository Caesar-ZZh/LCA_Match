import streamlit as st
import pandas as pd
import os
import re
import difflib
import io
import csv

# ================= ç½‘é¡µåŸºç¡€é…ç½® =================
st.set_page_config(page_title="LCA æ™ºèƒ½åŒ¹é…ç³»ç»Ÿ (V39)", page_icon="ğŸŒ±", layout="wide")

st.title("ğŸŒ± LCA æ™ºèƒ½åŒ¹é…ç³»ç»Ÿ (Webç‰ˆ)")
st.markdown("""
### ğŸš€ ä½¿ç”¨æŒ‡å—
1. **åå°æ•°æ®**ï¼šè¯·ç¡®ä¿æœåŠ¡å™¨ç«¯å·²åŠ è½½æ‰€æœ‰åŸºç¡€æ•°æ®åº“ï¼ˆå¤´è¡¨ã€ä¸Šæ¸¸è¡¨ç­‰ï¼‰ã€‚
2. **ä¸Šä¼ æ–‡ä»¶**ï¼šè¯·ä¸Šä¼ éœ€è¦åŒ¹é…çš„ **[æ¨¡å‹ç‰©æ–™é¡¹]** è¡¨æ ¼ï¼ˆæ”¯æŒ .xlsx æˆ– .csvï¼‰ã€‚
3. **è‡ªåŠ¨å¤„ç†**ï¼šç³»ç»Ÿå°†æ‰§è¡Œ V38 æ ¸å¿ƒç®—æ³•ï¼ˆå†·å´æ°´ä¿®æ­£ã€åŸºæœ¬æµå…œåº•ã€ä¸¥æ ¼åœ°ç†è¿‡æ»¤ï¼‰ã€‚
4. **ç»“æœä¸‹è½½**ï¼šåŒ¹é…å®Œæˆåï¼Œä¸‹è½½æ ‡å‡†æ ¼å¼ CSV æ–‡ä»¶ã€‚
""")

# ================= 0. åå°æ–‡ä»¶åŠ è½½å™¨ =================
@st.cache_data
def load_reference_data():
    files_map = {
        "å¤´è¡¨": "åŒ¹é…å…³ç³»å¤´è¡¨.CSV", 
        "ä¸Šæ¸¸è¡¨": "åŒ¹é…å…³ç³»ä¸Šæ¸¸èƒŒæ™¯æ•°æ®è¡Œè¡¨.CSV",
        "åŸºæœ¬æµè¡¨": "åŒ¹é…å…³ç³»åŸºæœ¬æµè¡¨.CSV",
        "åºŸå¼ƒç‰©è¡¨": "åŒ¹é…å…³ç³»åºŸå¼ƒç‰©å¤„ç½®èƒŒæ™¯æ•°æ®è¡Œè¡¨.CSV",
        "å‰¯äº§å“è¡¨": "åŒ¹é…å…³ç³»å‰¯äº§å“èƒŒæ™¯æ•°æ®è¡Œè¡¨.CSV",
        "å›æ”¶åˆ©ç”¨è¡¨": "åŒ¹é…å…³ç³»å›æ”¶åˆ©ç”¨èƒŒæ™¯æ•°æ®è¡Œè¡¨.CSV"
    }
    
    loaded = {}
    missing = []

    for key, fname in files_map.items():
        if os.path.exists(fname):
            try:
                loaded[key] = pd.read_csv(fname, dtype=str)
            except:
                try:
                    loaded[key] = pd.read_csv(fname, encoding='gbk', dtype=str)
                except:
                    try:
                        # è¿™é‡Œè¯»å–åå°å‚è€ƒè¡¨æ—¶ä¹ŸåŠ ä¸Š engine='openpyxl' ä»¥é˜²ä¸‡ä¸€
                        loaded[key] = pd.read_excel(fname, dtype=str, engine='openpyxl')
                    except:
                        pass 
        else:
            missing.append(fname)
    
    return loaded, missing

with st.spinner('æ­£åœ¨åŠ è½½åå°æ•°æ®åº“...'):
    ref_dfs, missing_files = load_reference_data()

st.sidebar.title("ğŸ“¦ æ•°æ®åº“çŠ¶æ€")
if missing_files:
    st.sidebar.error(f"âŒ ç¼ºå¤±æ–‡ä»¶: {len(missing_files)} ä¸ª")
    for f in missing_files:
        st.sidebar.text(f"- {f}")
    st.error("âš ï¸ ä¸¥é‡é”™è¯¯ï¼šåå°å‚è€ƒæ–‡ä»¶ç¼ºå¤±ï¼Œæ— æ³•è¿è¡ŒåŒ¹é…ï¼è¯·æ£€æŸ¥æ–‡ä»¶å¤¹ã€‚")
    st.stop()
else:
    st.sidebar.success("âœ… æ‰€æœ‰å‚è€ƒåº“åŠ è½½æ­£å¸¸")

# ================= 1. æ ¸å¿ƒç®—æ³• (V38é€»è¾‘) =================

def process_matching(df_model, ref_dfs):
    df_header = ref_dfs['å¤´è¡¨']
    bg_dfs = {
        'Upstream': ref_dfs['ä¸Šæ¸¸è¡¨'], 'Waste': ref_dfs['åºŸå¼ƒç‰©è¡¨'],
        'Byprod': ref_dfs['å‰¯äº§å“è¡¨'], 'Recycle': ref_dfs['å›æ”¶åˆ©ç”¨è¡¨'], 'Elementary': ref_dfs['åŸºæœ¬æµè¡¨']
    }

    progress_bar = st.progress(0, text="æ­£åœ¨ç´¢å¼•èƒŒæ™¯æ•°æ®...")
    
    h_name_col = next((c for c in df_header.columns if 'åç§°' in c and 'ä¸­æ–‡' in c), 'ç‰©æ–™é¡¹åç§°ï¼ˆä¸­æ–‡ï¼‰')
    h_id_col = next((c for c in df_header.columns if 'åŒ¹é…å…³ç³»ID' in c), 'åŒ¹é…å…³ç³»ID')
    df_header['clean'] = df_header[h_name_col].astype(str).str.strip()
    header_map = df_header.set_index('clean')[h_id_col].astype(str).str.strip().to_dict()
    
    bg_id_map = {}
    bg_name_list = {'Upstream': [], 'Waste': [], 'Byprod': [], 'Recycle': [], 'Elementary': []}
    
    total_cats = len(bg_dfs)
    curr = 0
    for cat, df in bg_dfs.items():
        lid_col = next((c for c in df.columns if 'åŒ¹é…å…³ç³»ID' in c), None)
        if cat == 'Elementary':
            name_col = next((c for c in df.columns if 'åŸºæœ¬æµåç§°' in c and 'ä¸­æ–‡' in c), 'åŸºæœ¬æµåç§°ï¼ˆä¸­æ–‡ï¼‰')
            unit_col = next((c for c in df.columns if 'å•ä½' in c), 'å•ä½ï¼ˆè‹±æ–‡ï¼‰')
            loc_col = next((c for c in df.columns if 'åˆ†ç±»' in c), 'åŸºæœ¬æµåˆ†ç±»') 
            fact_col, ref_col = None, None
        else:
            name_col = next((c for c in df.columns if 'åç§°' in c and 'ä¸­æ–‡' in c), 'åç§°')
            unit_col = next((c for c in df.columns if 'å•ä½' in c), 'å•ä½')
            loc_col = next((c for c in df.columns if 'åœ°ç†ä½ç½®' in c), 'åœ°ç†ä½ç½®')
            fact_col = next((c for c in df.columns if 'ç¢³è¶³è¿¹' in c), 'ç¢³è¶³è¿¹')
            ref_col = next((c for c in df.columns if 'å‚è€ƒäº§å“' in c), None)
        
        id_col = 'ID'
        for _, row in df.iterrows():
            item = {
                'ID': str(row.get(id_col, '')).strip(),
                'ç¢³è¶³è¿¹': str(row.get(fact_col, '')) if fact_col else "N/A",
                'å•ä½': str(row.get(unit_col, '')).strip(),
                'åœ°ç†ä½ç½®': str(row.get(loc_col, '')).strip(),
                'èƒŒæ™¯åç§°': str(row.get(name_col, '')).strip(),
                'å‚è€ƒäº§å“': str(row.get(ref_col, '')).strip() if ref_col else "",
                'æ¥æº': cat
            }
            if lid_col:
                lid = str(row[lid_col]).strip()
                if lid not in bg_id_map: bg_id_map[lid] = []
                bg_id_map[lid].append(item)
            bg_name_list[cat].append(item)
        curr += 1
        progress_bar.progress(int(curr/total_cats * 20), text="æ­£åœ¨ç´¢å¼•èƒŒæ™¯æ•°æ®...")

    STRICT_LOCATIONS = {
        'ä¸­å›½', 'cn', 'china', 'å…¨çƒ', 'glo', 'global',
        'row', 'rest of world', 'ä¸–ç•Œå…¶ä»–åœ°åŒº', 'æœªæŒ‡å®š', 'unspecified'
    }
    SYNONYMS_MAP = {
        'æ²³æ°´': ['åœ°è¡¨æ°´', 'surface water', 'water, river', 'æ²³', 'river'],
        'æ¹–æ°´': ['åœ°è¡¨æ°´', 'surface water', 'water, lake', 'æ¹–', 'lake'],
        'é›¨æ°´': ['åœ°è¡¨æ°´', 'surface water', 'water, rain', 'é›¨'],
        'å†·å´æ°´': ['è‡ªæ¥æ°´', 'tap water'], 'å¾ªç¯æ°´': ['è‡ªæ¥æ°´', 'tap water']
    }
    SPECIAL_RULES = {'ä¸€èˆ¬å·¥ä¸šå›ºåºŸ': '43274789141377048'}

    def clean_name_str(s): return re.sub(r'\(.*?\)|ï¼ˆ.*?ï¼‰', '', s).strip()
    def string_similarity(s1, s2): return difflib.SequenceMatcher(None, s1.lower(), s2.lower()).ratio()
    def check_unit(m_unit, bg_unit): return "ä¸€è‡´" if m_unit == bg_unit else "ä¸ä¸€è‡´"

    def get_score(item, m_name, m_cat):
        loc = item['åœ°ç†ä½ç½®']
        bg_name = item['èƒŒæ™¯åç§°'].lower()
        ref_prod = item['å‚è€ƒäº§å“'].lower()
        source = item['æ¥æº']
        m_name_clean = clean_name_str(m_name).lower()
        
        if 'å†·å´æ°´' in m_name or 'å¾ªç¯æ°´' in m_name:
            if ('è‡ªæ¥æ°´' in bg_name or 'tap water' in bg_name):
                return 999 if ('å¸‚åœº' in bg_name or 'market' in bg_name) else 500
        
        score = 10 
        if source == 'Elementary':
            score = 50
            if 'æœªæŒ‡å®š' in loc or 'unspecified' in loc.lower(): score += 30
            if 'æ°´' in m_name:
                if 'æœªæŒ‡å®šçš„å¤©ç„¶æ¥æº' in bg_name or 'unspecified natural origin' in bg_name: score += 20
                if 'åœ°è¡¨' in loc or 'surface' in loc.lower(): score += 15
                if 'æ²³' in m_name and 'river' in bg_name: score += 40 
                if 'æ¹–' in m_name and 'lake' in bg_name: score += 40 
            if m_cat == 'å¤§æ°”æ’æ”¾' and 'ç©ºæ°”' in loc: score += 10
            elif m_cat == 'æ°´ä½“æ’æ”¾' and 'æ°´' in loc: score += 10
            sim = string_similarity(m_name, item['èƒŒæ™¯åç§°'])
            score += sim * 5
            return score
        
        if 'hiq' in bg_name and loc=='ä¸­å›½': score = 100
        elif loc=='ä¸­å›½': score = 90
        elif 'ä¸–ç•Œå…¶ä»–åœ°åŒº' in loc or 'RoW' in loc: score = 80
        elif 'å…¨çƒ' in loc: score = 70
        
        if len(ref_prod) > 1 and (m_name_clean in ref_prod or ref_prod in m_name_clean): score += 20
        if any(k in bg_name for k in ['æœªæŒ‡å®š','unspecified','ä¸æŒ‡å®š','å¹³å‡','é€šç”¨','æ··åˆ']): score += 25
        if any(k in bg_name for k in ['ç”Ÿäº§','production','åˆ¶é€ ']): score += 10
        if m_cat in ['åºŸå¼ƒç‰©', 'å‰¯äº§å“']:
            whitelist = ['å¤„ç†','å¤„ç½®','ç„šçƒ§','å¡«åŸ‹','å›æ”¶','å†åˆ©ç”¨','treatment','disposal']
            if 'ç”Ÿäº§' in bg_name and not any(w in bg_name for w in whitelist): score -= 40
        
        sim = string_similarity(m_name_clean, clean_name_str(item['èƒŒæ™¯åç§°']).lower())
        score += sim * 10 
        return score

    result_data = []
    total_rows = len(df_model)
    progress_bar.progress(30, text="AI æ­£åœ¨åŒ¹é…ä¸­...")
    
    for idx, row in df_model.iterrows():
        if idx % 5 == 0:
            prog = 30 + int((idx / total_rows) * 60)
            progress_bar.progress(min(prog, 99), text=f"æ­£åœ¨åŒ¹é…: {row.get('ç‰©æ–™é¡¹åç§°ï¼ˆä¸­æ–‡ï¼‰', '')}")

        m_name = str(row.get('ç‰©æ–™é¡¹åç§°ï¼ˆä¸­æ–‡ï¼‰', '')).strip()
        m_cat = str(row.get('ç‰©æ–™é¡¹ç±»åˆ«', '')).strip()
        m_type = str(row.get('ç‰©æ–™é¡¹ç±»å‹', ''))
        m_attr = str(row.get('ç‰©æ–™é¡¹å±æ€§', ''))
        
        candidates = []
        if m_name in SPECIAL_RULES:
            cands = bg_id_map.get(SPECIAL_RULES[m_name])
            if cands: candidates.extend(cands)
        
        lid = header_map.get(m_name)
        if lid and lid in bg_id_map:
            cands = bg_id_map[lid]
            if m_cat in ['è‡ªç„¶èµ„æºè¾“å…¥', 'å¤§æ°”æ’æ”¾', 'æ°´ä½“æ’æ”¾']:
                candidates.extend([c for c in cands if c['æ¥æº'] == 'Elementary'])
            else:
                candidates.extend([c for c in cands if c['æ¥æº'] != 'Elementary'])
        
        search_terms = [m_name, clean_name_str(m_name)]
        if m_name in SYNONYMS_MAP: search_terms.extend(SYNONYMS_MAP[m_name])
        
        target_cats = []
        if m_cat in ['åŸè¾…æ–™', 'èƒ½æºåŠèƒ½æºä»‹è´¨']: target_cats = ['Upstream']
        elif m_cat == 'åºŸå¼ƒç‰©': target_cats = ['Waste']
        elif m_cat == 'å‰¯äº§å“': target_cats = ['Byprod']
        elif m_cat == 'å›æ”¶åˆ©ç”¨': target_cats = ['Recycle']
        elif m_cat in ['è‡ªç„¶èµ„æºè¾“å…¥', 'å¤§æ°”æ’æ”¾', 'æ°´ä½“æ’æ”¾']: target_cats = ['Elementary']
        
        is_natural = any(x in m_name for x in ['æ°´', 'æ²³', 'æ¹–', 'é›¨', 'äº•', 'æ°”', 'åœŸ', 'èµ„æº'])
        if is_natural or not candidates:
            if 'Elementary' not in target_cats: target