import streamlit as st
import pandas as pd
import os
import re
import difflib
import io
import csv

# ================= ç½‘é¡µåŸºç¡€é…ç½® =================
st.set_page_config(page_title="LCA æ™ºèƒ½åŒ¹é…ç³»ç»Ÿ (V42)", page_icon="ğŸŒ±", layout="wide")

st.title("ğŸŒ± LCA æ™ºèƒ½åŒ¹é…ç³»ç»Ÿ (Webç‰ˆ)")
st.markdown("""
### ğŸš€ ä½¿ç”¨æŒ‡å—
1. **åå°æ•°æ®**ï¼šè¯·ç¡®ä¿æœåŠ¡å™¨ç«¯å·²åŠ è½½æ‰€æœ‰åŸºç¡€æ•°æ®åº“ã€‚
2. **ä¸Šä¼ æ–‡ä»¶**ï¼šæ”¯æŒ **.xlsx** (Excel) å’Œ **.csv** æ ¼å¼ã€‚
3. **è‡ªåŠ¨å¤„ç†**ï¼šç³»ç»Ÿå°†è‡ªåŠ¨è¯†åˆ«ç¼–ç æ ¼å¼ï¼Œå¹¶æ‰§è¡Œ V38 æ ¸å¿ƒç®—æ³•ã€‚
""")

# ================= 0. åå°æ–‡ä»¶åŠ è½½å™¨ =================
@st.cache_data
def load_reference_data():
    required_map = {
        "å¤´è¡¨": "åŒ¹é…å…³ç³»å¤´è¡¨", 
        "ä¸Šæ¸¸è¡¨": "åŒ¹é…å…³ç³»ä¸Šæ¸¸èƒŒæ™¯æ•°æ®è¡Œè¡¨",
        "åŸºæœ¬æµè¡¨": "åŒ¹é…å…³ç³»åŸºæœ¬æµè¡¨",
        "åºŸå¼ƒç‰©è¡¨": "åŒ¹é…å…³ç³»åºŸå¼ƒç‰©å¤„ç½®èƒŒæ™¯æ•°æ®è¡Œè¡¨",
        "å‰¯äº§å“è¡¨": "åŒ¹é…å…³ç³»å‰¯äº§å“èƒŒæ™¯æ•°æ®è¡Œè¡¨",
        "å›æ”¶åˆ©ç”¨è¡¨": "åŒ¹é…å…³ç³»å›æ”¶åˆ©ç”¨èƒŒæ™¯æ•°æ®è¡Œè¡¨"
    }
    
    loaded = {}
    missing = []
    
    try:
        all_files_on_disk = os.listdir('.')
    except:
        all_files_on_disk = []
        
    file_index = {f.lower(): f for f in all_files_on_disk}

    for key, core_name in required_map.items():
        candidates = [f"{core_name}.csv".lower(), f"{core_name}.xlsx".lower()]
        found_real_name = None
        for cand in candidates:
            if cand in file_index:
                found_real_name = file_index[cand]
                break
        
        if found_real_name:
            try:
                if found_real_name.lower().endswith('.csv'):
                    try:
                        loaded[key] = pd.read_csv(found_real_name, dtype=str)
                    except:
                        # è‡ªåŠ¨å°è¯• GBK è¯»å–åå° CSV
                        loaded[key] = pd.read_csv(found_real_name, encoding='gbk', dtype=str)
                else:
                    # è¯»å–åå° Excel ä¹Ÿè¦æŒ‡å®šå¼•æ“
                    loaded[key] = pd.read_excel(found_real_name, dtype=str, engine='openpyxl')
            except Exception as e:
                missing.append(f"{found_real_name} (æŸå: {str(e)})")
        else:
            missing.append(core_name)
    
    return loaded, missing

with st.spinner('æ­£åœ¨åŠ è½½åå°æ•°æ®åº“...'):
    ref_dfs, missing_files = load_reference_data()

st.sidebar.title("ğŸ“¦ æ•°æ®åº“çŠ¶æ€")
if missing_files:
    st.sidebar.warning(f"âš ï¸ åå°æ–‡ä»¶ç¼ºå¤±: {len(missing_files)} ä¸ª")
else:
    st.sidebar.success("âœ… æ‰€æœ‰å‚è€ƒåº“åŠ è½½æ­£å¸¸")

# ================= 1. æ ¸å¿ƒç®—æ³• (V38é€»è¾‘) =================

def process_matching(df_model, ref_dfs):
    if len(ref_dfs) < 6:
        return [], ["é”™è¯¯ï¼šåå°å‚è€ƒæ–‡ä»¶ä¸å…¨"], None

    df_header = ref_dfs['å¤´è¡¨']
    bg_dfs = {
        'Upstream': ref_dfs['ä¸Šæ¸¸è¡¨'], 'Waste': ref_dfs['åºŸå¼ƒç‰©è¡¨'],
        'Byprod': ref_dfs['å‰¯äº§å“è¡¨'], 'Recycle': ref_dfs['å›æ”¶åˆ©ç”¨è¡¨'], 'Elementary': ref_dfs['åŸºæœ¬æµè¡¨']
    }

    progress_bar = st.progress(0, text="æ­£åœ¨ç´¢å¼•èƒŒæ™¯æ•°æ®...")
    
    h_name_col = next((c for c in df_header.columns if 'åç§°' in c and 'ä¸­æ–‡' in c), 'ç‰©æ–™é¡¹åç§°ï¼ˆä¸­æ–‡ï¼‰')
    h_id_col = next((c for c in df_header.columns if 'åŒ¹é…å…³ç³»ID' in c), 'åŒ¹é…å…³ç³»ID')
    df_header['clean'] = df_header[h_name_col].astype(str).str.strip()
    header_map = df_header.set_index('clean')[h_id_col].astype(str).str.strip().to_dict()
    
    bg_id_map = {}
    bg_name_list = {'Upstream': [], 'Waste': [], 'Byprod': [], 'Recycle': [], 'Elementary': []}
    
    total_cats = len(bg_dfs)
    curr = 0
    for cat, df in bg_dfs.items():
        lid_col = next((c for c in df.columns if 'åŒ¹é…å…³ç³»ID' in c), None)
        if cat == 'Elementary':
            name_col = next((c for c in df.columns if 'åŸºæœ¬æµåç§°' in c and 'ä¸­æ–‡' in c), 'åŸºæœ¬æµåç§°ï¼ˆä¸­æ–‡ï¼‰')
            unit_col = next((c for c in df.columns if 'å•ä½' in c), 'å•ä½ï¼ˆè‹±æ–‡ï¼‰')
            loc_col = next((c for c in df.columns if 'åˆ†ç±»' in c), 'åŸºæœ¬æµåˆ†ç±»') 
            fact_col, ref_col = None, None
        else:
            name_col = next((c for c in df.columns if 'åç§°' in c and 'ä¸­æ–‡' in c), 'åç§°')
            unit_col = next((c for c in df.columns if 'å•ä½' in c), 'å•ä½')
            loc_col = next((c for c in df.columns if 'åœ°ç†ä½ç½®' in c), 'åœ°ç†ä½ç½®')
            fact_col = next((c for c in df.columns if 'ç¢³è¶³è¿¹' in c), 'ç¢³è¶³è¿¹')
            ref_col = next((c for c in df.columns if 'å‚è€ƒäº§å“' in c), None)
        
        id_col = 'ID'
        for _, row in df.iterrows():
            item = {
                'ID': str(row.get(id_col, '')).strip(),
                'ç¢³è¶³è¿¹': str(row.get(fact_col, '')) if fact_col else "N/A",
                'å•ä½': str(row.get(unit_col, '')).strip(),
                'åœ°ç†ä½ç½®': str(row.get(loc_col, '')).strip(),
                'èƒŒæ™¯åç§°': str(row.get(name_col, '')).strip(),
                'å‚è€ƒäº§å“': str(row.get(ref_col, '')).strip() if ref_col else "",
                'æ¥æº': cat
            }
            if lid_col:
                lid = str(row[lid_col]).strip()
                if lid not in bg_id_map: bg_id_map[lid] = []
                bg_id_map[lid].append(item)
            bg_name_list[cat].append(item)
        curr += 1
        progress_bar.progress(int(curr/total_cats * 20), text="æ­£åœ¨ç´¢å¼•èƒŒæ™¯æ•°æ®...")

    STRICT_LOCATIONS = {
        'ä¸­å›½', 'cn', 'china', 'å…¨çƒ', 'glo', 'global',
        'row', 'rest of world', 'ä¸–ç•Œå…¶ä»–åœ°åŒº', 'æœªæŒ‡å®š', 'unspecified'
    }
    SYNONYMS_MAP = {
        'æ²³æ°´': ['åœ°è¡¨æ°´', 'surface water', 'water, river', 'æ²³', 'river'],
        'æ¹–æ°´': ['åœ°è¡¨æ°´', 'surface water', 'water, lake', 'æ¹–', 'lake'],
        'é›¨æ°´': ['åœ°è¡¨æ°´', 'surface water', 'water, rain', 'é›¨'],
        'å†·å´æ°´': ['è‡ªæ¥æ°´', 'tap water'], 'å¾ªç¯æ°´': ['è‡ªæ¥æ°´', 'tap water']
    }
    SPECIAL_RULES = {'ä¸€èˆ¬å·¥ä¸šå›ºåºŸ': '43274789141377048'}

    def clean_name_str(s): return re.sub(r'\(.*?\)|ï¼ˆ.*?ï¼‰', '', s).strip()
    def string_similarity(s1, s2): return difflib.SequenceMatcher(None, s1.lower(), s2.lower()).ratio()
    def check_unit(m_unit, bg_unit): return "ä¸€è‡´" if m_unit == bg_unit else "ä¸ä¸€è‡´"

    def get_score(item, m_name, m_cat):
        loc = item['åœ°ç†ä½ç½®']
        bg_name = item['èƒŒæ™¯åç§°'].lower()
        ref_prod = item['å‚è€ƒäº§å“'].lower()
        source = item['æ¥æº']
        m_name_clean = clean_name_str(m_name).lower()
        
        if 'å†·å´æ°´' in m_name or 'å¾ªç¯æ°´' in m_name:
            if ('è‡ªæ¥æ°´' in bg_name or 'tap water' in bg_name):
                return 999 if ('å¸‚åœº' in bg_name or 'market' in bg_name) else 500
        
        score = 10 
        if source == 'Elementary':
            score = 50
            if 'æœªæŒ‡å®š' in loc or 'unspecified' in loc.lower(): score += 30
            if 'æ°´' in m_name:
                if 'æœªæŒ‡å®šçš„å¤©ç„¶æ¥æº' in bg_name or 'unspecified natural origin' in bg_name: score += 20
                if 'åœ°è¡¨' in loc or 'surface' in loc.lower(): score += 15
                if 'æ²³' in m_name and 'river' in bg_name: score += 40 
                if 'æ¹–' in m_name and 'lake' in bg_name: score += 40 
            if m_cat == 'å¤§æ°”æ’æ”¾' and 'ç©ºæ°”' in loc: score += 10
            elif m_cat == 'æ°´ä½“æ’æ”¾' and 'æ°´' in loc: score += 10
            sim = string_similarity(m_name, item['èƒŒæ™¯åç§°'])
            score += sim * 5
            return score
        
        if 'hiq' in bg_name and loc=='ä¸­å›½': score = 100
        elif loc=='ä¸­å›½': score = 90
        elif 'ä¸–ç•Œå…¶ä»–åœ°åŒº' in loc or 'RoW' in loc: score = 80
        elif 'å…¨çƒ' in loc: score = 70
        
        if len(ref_prod) > 1 and (m_name_clean in ref_prod or ref_prod in m_name_clean): score += 20
        if any(k in bg_name for k in ['æœªæŒ‡å®š','unspecified','ä¸æŒ‡å®š','å¹³å‡','é€šç”¨','æ··åˆ']): score += 25
        if any(k in bg_name for k in ['ç”Ÿäº§','production','åˆ¶é€ ']): score += 10
        if m_cat in ['åºŸå¼ƒç‰©', 'å‰¯äº§å“']:
            whitelist = ['å¤„ç†','å¤„ç½®','ç„šçƒ§','å¡«åŸ‹','å›æ”¶','å†åˆ©ç”¨','treatment','disposal']
            if 'ç”Ÿäº§' in bg_name and not any(w in bg_name for w in whitelist): score -= 40
        
        sim = string_similarity(m_name_clean, clean_name_str(item['èƒŒæ™¯åç§°']).lower())
        score += sim * 10 
        return score

    result_data = []
    total_rows = len(df_model)
    progress_bar.progress(30, text="AI æ­£åœ¨åŒ¹é…ä¸­...")
    
    for idx, row in df_model.iterrows():
        if idx % 5 == 0:
            prog = 30 + int((idx / total_rows) * 60)
            progress_bar.progress(min(prog, 99), text=f"æ­£åœ¨åŒ¹é…: {row.get('ç‰©æ–™é¡¹åç§°ï¼ˆä¸­æ–‡ï¼‰', '')}")

        m_name = str(row.get('ç‰©æ–™é¡¹åç§°ï¼ˆä¸­æ–‡ï¼‰', '')).strip()
        m_cat = str(row.get('ç‰©æ–™é¡¹ç±»åˆ«', '')).strip()
        m_type = str(row.get('ç‰©æ–™é¡¹ç±»å‹', ''))
        m_attr = str(row.get('ç‰©æ–™é¡¹å±æ€§', ''))
        
        candidates = []
        if m_name in SPECIAL_RULES:
            cands = bg_id_map.get(SPECIAL_RULES[m_name])
            if cands: candidates.extend(cands)
        
        lid = header_map.get(m_name)
        if lid and lid in bg_id_map:
            cands = bg_id_map[lid]
            if m_cat in ['è‡ªç„¶èµ„æºè¾“å…¥', 'å¤§æ°”æ’æ”¾', 'æ°´ä½“æ’æ”¾']:
                candidates.extend([c for c in cands if c['æ¥æº'] == 'Elementary'])
            else:
                candidates.extend([c for c in cands if c['æ¥æº'] != 'Elementary'])
        
        search_terms = [m_name, clean_name_str(m_name)]
        if m_name in SYNONYMS_MAP: search_terms.extend(SYNONYMS_MAP[m_name])
        
        target_cats = []
        if m_cat in ['åŸè¾…æ–™', 'èƒ½æºåŠèƒ½æºä»‹è´¨']: target_cats = ['Upstream']
        elif m_cat == 'åºŸå¼ƒç‰©': target_cats = ['Waste']
        elif m_cat == 'å‰¯äº§å“': target_cats = ['Byprod']
        elif m_cat == 'å›æ”¶åˆ©ç”¨': target_cats = ['Recycle']
        elif m_cat in ['è‡ªç„¶èµ„æºè¾“å…¥', 'å¤§æ°”æ’æ”¾', 'æ°´ä½“æ’æ”¾']: target_cats = ['Elementary']
        
        is_natural = any(x in m_name for x in ['æ°´', 'æ²³', 'æ¹–', 'é›¨', 'äº•', 'æ°”', 'åœŸ', 'èµ„æº'])
        if is_natural or not candidates:
            if 'Elementary' not in target_cats: target_cats.append('Elementary')

        for cat in target_cats:
            for item in bg_name_list[cat]:
                bg_name = item['èƒŒæ™¯åç§°'].lower()
                for term in search_terms:
                    if term.lower() in bg_name:
                        candidates.append(item)
                        break

        if candidates:
            unique_candidates = {c['ID']: c for c in candidates}.values()
            filtered = [c for c in unique_candidates if str(c['åœ°ç†ä½ç½®']).strip().lower() in STRICT_LOCATIONS]
            candidates = filtered
            candidates.sort(key=lambda x: get_score(x, m_name, m_cat), reverse=True)
            
            for i, cand in enumerate(candidates):
                is_default = (i == 0)
                is_hiq = 'hiq' in cand['èƒŒæ™¯åç§°'].lower()
                
                row_data = [
                    m_type if is_default else "", m_attr if is_default else "", m_name if is_default else "",
                    cand['ID'],
                    "" if is_hiq else cand['èƒŒæ™¯åç§°'], "" if is_hiq else cand['å‚è€ƒäº§å“'],
                    "" if is_hiq else cand['åœ°ç†ä½ç½®'], "" if is_hiq else cand['å•ä½'],
                    cand['èƒŒæ™¯åç§°'] if is_hiq else "", cand['å‚è€ƒäº§å“'] if is_hiq else "",
                    cand['åœ°ç†ä½ç½®'] if is_hiq else "", cand['å•ä½'] if is_hiq else "",
                    "", ""
                ]
                result_data.append(row_data)
        else:
            row_data = [m_type, m_attr, m_name, "âŒ æ— åŒ¹é…", "", "", "", "", "", "", "", "", "", ""]
            result_data.append(row_data)
            
    progress_bar.progress(100, text="å®Œæˆï¼")
    
    FINAL_HEADERS = [
        'ç‰©æ–™é¡¹ç±»å‹', 'ç‰©æ–™é¡¹å±æ€§', '*ç‰©æ–™é¡¹åç§°ï¼ˆä¸­æ–‡ï¼‰', 'èƒŒæ™¯æ•°æ®/åŸºæœ¬æµID\nï¼ˆç‰¹æ®Šç‰©æ–™é¡¹éœ€åŒ¹é…ç‰¹æ®Šèµ„æºæŒ‡æ ‡ï¼‰',
        'é»˜è®¤èƒŒæ™¯æ•°æ®åç§°(ECO)\nï¼ˆæµåç§°è‹±æ–‡ï¼‰\nï¼ˆé¦–é€‰é‚£æ¡é«˜äº®æ˜¾ç¤ºï¼‰', 'å‚è€ƒäº§å“\nï¼ˆåŸºæœ¬æµåˆ†ç±»ï¼‰', 'åœ°åŒº', 'èƒŒæ™¯æ•°æ®å•ä½',
        'é»˜è®¤èƒŒæ™¯æ•°æ®åç§°(HIQ)\nï¼ˆé¦–é€‰é‚£æ¡é«˜äº®æ˜¾ç¤ºï¼‰', 'å‚è€ƒäº§å“', 'åœ°åŒº', 'èƒŒæ™¯æ•°æ®å•ä½',
        'è´Ÿè´£äºº', 'å®¡æ ¸æ„è§'
    ]
    return result_data, FINAL_HEADERS, None

# ================= 2. ç”¨æˆ·äº¤äº’ç•Œé¢ (V42: èåˆä¿®å¤ç‰ˆ) =================

uploaded_file = st.file_uploader("ğŸ“‚ ç‚¹å‡»æ­¤å¤„ä¸Šä¼ æ¨¡å‹è¡¨", type=['xlsx', 'csv'])

if uploaded_file:
    try:
        # ğŸ”¥ V42 æ ¸å¿ƒä¿®å¤é€»è¾‘ ğŸ”¥
        
        # æƒ…å†µ1: å¦‚æœæ˜¯ CSV æ–‡ä»¶
        if uploaded_file.name.lower().endswith('.csv'):
            try:
                # ä¼˜å…ˆå°è¯• utf-8 è¯»å–
                df_input = pd.read_csv(uploaded_file, dtype=str)
            except UnicodeDecodeError:
                # å¦‚æœ utf-8 å¤±è´¥ (0xb2 error), é‡ç½®æŒ‡é’ˆå¹¶å°è¯• GBK
                uploaded_file.seek(0)
                df_input = pd.read_csv(uploaded_file, dtype=str, encoding='gbk')
        
        # æƒ…å†µ2: å¦‚æœæ˜¯ Excel æ–‡ä»¶
        else:
            # å¿…é¡»æŒ‡å®š engine='openpyxl' (å‰ææ˜¯å·²å®‰è£… pip install openpyxl)
            df_input = pd.read_excel(uploaded_file, dtype=str, engine='openpyxl')
        
        st.info(f"ğŸ“„ æˆåŠŸè¯»å–: {uploaded_file.name}, å…± {len(df_input)} è¡Œ")
        
        if st.button("ğŸš€ å¼€å§‹è¿è¡ŒåŒ¹é…", type="primary"):
            if missing_files:
                st.error("æ— æ³•è¿è¡Œï¼šåå°ç¼ºå°‘å¿…è¦çš„å‚è€ƒæ•°æ®åº“ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ä¸Šä¼ ã€‚")
            else:
                result_data, headers, err = process_matching(df_input, ref_dfs)
                
                st.success("ğŸ‰ åŒ¹é…æˆåŠŸï¼")
                
                csv_buffer = io.StringIO()
                writer = csv.writer(csv_buffer)
                writer.writerow(headers)
                writer.writerows(result_data)
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½æœ€ç»ˆç»“æœ (CSV)",
                    data=csv_buffer.getvalue().encode('utf-8-sig'),
                    file_name="LCA_åŒ¹é…ç»“æœ_V42.csv",
                    mime="text/csv"
                )
                
                with st.expander("ğŸ‘ï¸ ç‚¹å‡»æŸ¥çœ‹ç»“æœé¢„è§ˆ"):
                    st.dataframe(pd.DataFrame(result_data, columns=[h.replace('\n','') for h in headers]).head(50))

    except Exception as e:
        st.error(f"âŒ æ–‡ä»¶è§£æå¤±è´¥: {e}")